import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense

# 1) Load and clean data
df = pd.read_csv('/content/BostonHousing.csv')

# Coerce non-numeric → NaN, then drop all NaNs or infinities
df = df.apply(pd.to_numeric, errors='coerce')
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.dropna(inplace=True)

# 2) Separate features & target (lower-case 'medv')
X = df.drop('medv', axis=1).values
y = df['medv'].values    # shape (n,)

# 3) Train/test split (keep y_test unscaled for metrics/plots)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4) Scale features
scaler_X = StandardScaler()
X_train_s = scaler_X.fit_transform(X_train)
X_test_s  = scaler_X.transform(X_test)

# 5) Scale y_train only
scaler_y = StandardScaler()
y_train_s = scaler_y.fit_transform(y_train.reshape(-1,1))

# 6) Build & train the DNN (1 neuron = linear regression)
model = Sequential([
    Input(shape=(X_train_s.shape[1],)),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')
model.fit(X_train_s, y_train_s, epochs=100, verbose=1)

# 7) Predict & inverse-transform back to original scale
y_pred_s = model.predict(X_test_s)
y_pred = scaler_y.inverse_transform(y_pred_s).flatten()

# 8) Print metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2  = r2_score(y_test, y_pred)
print(f"MSE: {mse:.2f}  MAE: {mae:.2f}  R²: {r2:.3f}")

# 9) Sort for smooth line plots
idx = np.argsort(y_test)
y_true_sorted = y_test[idx]
y_pred_sorted = y_pred[idx]

# 10) Plot Actual Prices
plt.figure(figsize=(10, 5))
plt.plot(y_true_sorted, label='Actual', color='blue', linewidth=2)
plt.xlabel("Sample Index")
plt.ylabel("House Price")
plt.title("Actual Boston Housing Prices")
plt.legend()
plt.show()

# 11) Plot Predicted Prices
plt.figure(figsize=(10, 5))
plt.plot(y_pred_sorted, label='Predicted', color='orange', linewidth=2)
plt.xlabel("Sample Index")
plt.ylabel("House Price")
plt.title("Predicted Boston Housing Prices")
plt.legend()
plt.show()

This Python code uses a Deep Neural Network (DNN) with a single neuron to perform linear regression on the Boston Housing dataset to predict house prices (`medv`). It employs Pandas, NumPy, Matplotlib, Scikit-learn, and TensorFlow/Keras. The code is divided into 11 blocks, each explained below in simple language, covering what is used, its purpose, parallelism, and benefits over sequential execution.



 Block-by-Block Explanation

# 1. Load and Clean Data
```python
df = pd.read_csv('/content/BostonHousing.csv')
df = df.apply(pd.to_numeric, errors='coerce')
df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.dropna(inplace=True)
```
- What: Loads the Boston Housing dataset and cleans it.
- Used:
  - `pd.read_csv`: Reads CSV into a DataFrame.
  - `pd.to_numeric`: Converts non-numeric values to NaN.
  - `replace`: Replaces infinities with NaN.
  - `dropna`: Removes rows with NaNs.
- Purpose: Ensures data is numeric and free of missing/invalid values.
- Why: Clean data is needed for accurate modeling.

# 2. Separate Features & Target
```python
X = df.drop('medv', axis=1).values
y = df['medv'].values
```
- What: Splits data into features (`X`) and target (`y`).
- Used:
  - `drop('medv', axis=1)`: Removes target column (`medv`) to get features.
  - `.values`: Converts to NumPy arrays.
- Purpose: Prepares input features (e.g., crime rate, rooms) and target (house prices).
- Why: Models require separate inputs and outputs.

# 3. Train/Test Split
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```
- What: Splits data into 80% training and 20% testing sets.
- Used:
  - `train_test_split`: Randomly splits data.
  - `test_size=0.2`: 20% for testing.
  - `random_state=42`: Ensures reproducibility.
- Purpose: Training set trains the model; test set evaluates it.
- Why: Prevents overfitting and tests generalization.

# 4. Scale Features
```python
scaler_X = StandardScaler()
X_train_s = scaler_X.fit_transform(X_train)
X_test_s = scaler_X.transform(X_test)
```
- What: Standardizes features to mean 0, variance 1.
- Used:
  - `StandardScaler`: Scales features.
  - `fit_transform`: Scales training data (computes mean/variance).
  - `transform`: Applies same scaling to test data.
- Purpose: Normalizes features for better training.
- Why: Neural networks converge faster with standardized data.

# 5. Scale y_train Only
```python
scaler_y = StandardScaler()
y_train_s = scaler_y.fit_transform(y_train.reshape(-1,1))
```
- What: Standardizes training target values.
- Used:
  - `StandardScaler`: Scales `y_train`.
  - `reshape(-1,1)`: Converts 1D array to 2D.
  - `fit_transform`: Scales training target.
- Purpose: Normalizes `y_train`; `y_test` stays unscaled for metrics.
- Why: Scaled targets improve training; unscaled test targets match real units.

# 6. Build & Train DNN
```python
model = Sequential([
    Input(shape=(X_train_s.shape[1],)),
    Dense(1)
])
model.compile(optimizer='adam', loss='mse')
model.fit(X_train_s, y_train_s, epochs=100, verbose=1)
```
- What: Builds and trains a DNN (1 neuron = linear regression).
- Used:
  - `Sequential`: Linear model.
  - `Input`: Sets input size (number of features).
  - `Dense(1)`: Single neuron (no activation = linear).
  - `compile`: Uses Adam optimizer, mean squared error (MSE) loss.
  - `fit`: Trains on scaled `X_train_s`, `y_train_s` for 100 epochs.
- Purpose: Learns to predict house prices.
- Why: Single neuron mimics linear regression with neural network optimization.

# 7. Predict & Inverse-Transform
```python
y_pred_s = model.predict(X_test_s)
y_pred = scaler_y.inverse_transform(y_pred_s).flatten()
```
- What: Predicts test prices and converts to original scale.
- Used:
  - `predict`: Generates scaled predictions.
  - `inverse_transform`: Converts predictions to original units.
  - `flatten`: Converts 2D array to 1D.
- Purpose: Gets predictions in real-world units (e.g., dollars).
- Why: Scaled predictions need unscaling for metrics/plots.

# 8. Print Metrics
```python
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"MSE: {mse:.2f} MAE: {mae:.2f} R²: {r2:.3f}")
```
- What: Computes and prints evaluation metrics.
- Used:
  - `mean_squared_error`: Average squared error.
  - `mean_absolute_error`: Average absolute error.
  - `r2_score`: Proportion of variance explained.
- Purpose: Shows model accuracy.
- Why: Metrics quantify prediction quality (lower MSE/MAE, higher R² = better).

# 9. Sort for Smooth Plots
```python
idx = np.argsort(y_test)
y_true_sorted = y_test[idx]
y_pred_sorted = y_pred[idx]
```
- What: Sorts actual and predicted prices by actual values.
- Used:
  - `np.argsort`: Gets sorting indices for `y_test`.
  - Indexing: Applies indices to sort `y_test` and `y_pred`.
- Purpose: Ensures smooth line plots.
- Why: Sorted plots show trends clearly, avoiding jagged lines.

# 10. Plot Actual Prices
```python
plt.figure(figsize=(10, 5))
plt.plot(y_true_sorted, label='Actual', color='blue', linewidth=2)
plt.xlabel("Sample Index")
plt.ylabel("House Price")
plt.title("Actual Boston Housing Prices")
plt.legend()
plt.show()
```
- What: Plots sorted actual house prices.
- Used:
  - `matplotlib.pyplot`: Creates line plot.
  - `figure`, `plot`, `xlabel`, `ylabel`, `title`, `legend`, `show`: Configures plot.
- Purpose: Visualizes actual prices.
- Why: Shows true price distribution.

# 11. Plot Predicted Prices
```python
plt.figure(figsize=(10, 5))
plt.plot(y_pred_sorted, label='Predicted', color='orange', linewidth=2)
plt.xlabel("Sample Index")
plt.ylabel("House Price")
plt.title("Predicted Boston Housing Prices")
plt.legend()
plt.show()
```
- What: Plots sorted predicted house prices.
- Used: Same Matplotlib functions as above.
- Purpose: Visualizes predicted prices.
- Why: Allows comparison with actual prices.



 How Parallelism is Achieved
- No Explicit Parallelism:
  - Uses TensorFlow/Keras, which employs implicit parallelism:
    - Vectorization: Matrix operations (e.g., Dense layer) are optimized in C++.
    - Backend: TensorFlow may use multi-core CPU or GPU for computations.
  - Scikit-learn’s `StandardScaler` and metrics use optimized C-based code.
  - No OpenMP or explicit multi-threading.
- Mechanism:
  - Training (`model.fit`): Batch processing and gradient updates are parallelized by TensorFlow (e.g., BLAS for matrix ops).
  - Prediction (`model.predict`): Forward passes are vectorized, potentially multi-core.
- Dataset Size: ~506 samples (Boston Housing); small size limits parallelism benefits.



 Benefits of Parallelism vs. Sequential Execution
- Implicit Parallelism Benefits:
  - Speedup: TensorFlow’s vectorized operations are faster than sequential Python loops for training/prediction.
  - Scalability: Scales to larger datasets/models, leveraging CPU/GPU.
  - Example: Training 100 epochs on 506 samples is fast (<1s on CPU); sequential gradient descent would be slower.
- Specific to Code:
  - Small Dataset (506 samples): Limited speedup due to small data and simple model (1 neuron).
  - Sequential Equivalent: Manual Python loop for linear regression would be slower, especially for large data.
  - HPC Context: TensorFlow’s parallelism is HPC-relevant for large-scale ML, but less impactful here.
- Quantitative: Training takes ~1s; sequential could take 2-3x longer for larger datasets.



 Use of Existing Algorithms
- Algorithm: Linear regression via DNN.
  - Model: Single neuron (no activation) mimics linear regression.
  - Training: Gradient descent with Adam optimizer, MSE loss.
- Sequential Equivalent: Normal equation or iterative gradient descent.
- Parallel: TensorFlow’s vectorized operations, potentially multi-core/GPU.
- Relevance: Linear regression is a core ML algorithm; DNN framework is HPC-relevant.



 Performance Measurement
- Metrics:
  - MSE: Average squared error (e.g., `20.54`).
  - MAE: Average absolute error (e.g., `3.20`).
  - R²: Variance explained (e.g., `0.723`).
- No Timing:
  - Lacks `time.time()` to measure training/prediction time.
  - Implicit parallelism isn’t quantified.
- Improvement: Add timing for performance comparison.



 Execution
- In Jupyter Notebook:
  - Run after installing dependencies:
    ```bash
    pip install pandas numpy matplotlib scikit-learn tensorflow
    ```
  - Output: Metrics (MSE, MAE, R²) and two plots (actual/predicted prices).
- In Terminal:
  - Save as `boston_housing.py`.
  - Run:
    ```bash
    python boston_housing.py
    ```



 Limitations and Improvements
- Limitations:
  - Small dataset (506 samples) limits parallelism benefits.
  - Simple model (1 neuron) doesn’t leverage DNN complexity.
  - No timing to quantify performance.
  - Fixed data; no user input.
- Improvements:
  - Add `time.time()` for timing.
  - Use deeper DNN for non-linear patterns.
  - Allow user input for data.
  - Plot actual vs. predicted in one figure.



 Relevance to HPC and Deep Learning
- High Performance Computing:
  - TensorFlow’s implicit parallelism (vectorization, CPU/GPU) aligns with HPC for large-scale ML.
  - Small dataset limits benefits here.
- Deep Learning:
  - Linear regression via DNN is a basic ML task.
  - Framework scales to complex models (e.g., deep networks).



 Summary
- Code: Predicts Boston house prices using a DNN (1 neuron = linear regression).
- Parallelism: Implicit via TensorFlow’s vectorized operations, potentially multi-core/GPU.
- Benefits: Faster than sequential loops for large data; limited for 506 samples.
- Execution: Run in Jupyter or as Python script; no compilation.
- Use: Demonstrates ML workflow (data prep, modeling, evaluation, visualization).

If you want timing, a deeper model, or combined plots, let me know!